{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the required packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PGmRwBRLt1Zv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/unnimaya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing packages for pre-processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Packages for embeddings\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#Importing packages for cleaning the data\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Packages for embeddings\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Importing packages for deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Importing packages for visualization of results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydksZwWAj9Kt",
    "outputId": "f77bda3a-b115-4407-e89a-a7e76f699cab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to dataset\n",
    "dataset_path = '/Users/unnimaya/Documents/Projectexperiments/AIRLINEDATASET/usairlinetweets.csv'\n",
    "\n",
    "# Load the dataset in the path using pandas\n",
    "df_airline = pd.read_csv(dataset_path)\n",
    "\n",
    "# Display the initial rows of the dataframe\n",
    "df_airline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "B31SQQSikNuG",
    "outputId": "ccca8f18-918e-4fc2-c7e4-93b9131d1693"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_airline[[\"airline_sentiment\", \"text\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "GPT-iVxBksLb",
    "outputId": "0d4e7102-6664-47d4-d51f-59bb4d7f63be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment\n",
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"airline_sentiment\"].value_counts()\n",
    "#imbalanced data set\n",
    "#can use resampling techniques like k fold cross validation for better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the data\n",
    "lemmatizer = WordNetLemmatizer()#Initialize lemmatizer\n",
    "stop_words = set(stopwords.words('english'))#Initialize stopwords\n",
    "\n",
    "def text_cleaning(text):\n",
    "    text = text.lower()#Converting the text to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)#Punctuation removal using regular expression\n",
    "    text = re.sub(r'\\d+', '', text)#Number removal\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]#Stop word removal and lemmatization\n",
    "    cleaned_text = ' '.join(tokens)#Joins the tokens to form cleaned sentence   \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST DATA CREATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AuV1f6g2DOyQ",
    "outputId": "4057a000-8033-4709-aaef-0a8a7da900fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set class distribution:\n",
      " airline_sentiment\n",
      "negative    200\n",
      "neutral     200\n",
      "positive    200\n",
      "Name: count, dtype: int64\n",
      "Train set class distribution:\n",
      " airline_sentiment\n",
      "negative    8978\n",
      "neutral     2899\n",
      "positive    2163\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# The column 'airline_sentiment' is label\n",
    "class_label = 'airline_sentiment'\n",
    "\n",
    "# Sample 200 datapoints from each class for the test set\n",
    "test_df_class_1 = df[df[class_label] == 'negative'].sample(n=200, random_state=42)  # for class negative\n",
    "test_df_class_2 = df[df[class_label] == 'neutral'].sample(n=200, random_state=42)   # for class neutral\n",
    "test_df_class_3 = df[df[class_label] == 'positive'].sample(n=200, random_state=42)  # for class positive\n",
    "\n",
    "# Concatenate test_df_class_1, test_df_class_2 and test_df_class_3 to create the test set of 600 datapoints\n",
    "test_df = pd.concat([test_df_class_1, test_df_class_2, test_df_class_3])\n",
    "\n",
    "# original dataframe minus test_data = remaining data\n",
    "remaining_df = df.drop(test_df.index)\n",
    "\n",
    "# Display the number of samples in each class for both train and test sets\n",
    "# Check if the number of datapoints in remaining_df + test_df equals length of original dataframe\n",
    "print(\"Test set class distribution:\\n\", test_df[class_label].value_counts())\n",
    "print(\"Train set class distribution:\\n\", remaining_df[class_label].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FRjT5W4ScfI5"
   },
   "outputs": [],
   "source": [
    "# within test_df, \"text\" column is X and \"airline_sentiment\" column is Y\n",
    "# These will be referred to as X_test and y_test\n",
    "X_test= test_df[\"text\"]\n",
    "y_test= test_df[\"airline_sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVpEjzFXQYk_",
    "outputId": "a2635892-a971-4e82-96f4-ce099fd06039"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([ 1151,  9111,  3326, 10891, 11322,  3709,  5261, 10060, 12446,  3089,\n",
       "       ...\n",
       "       11705, 13422,  2716,  3071, 13504,  4489,  8271, 11795,  7294,  8203],\n",
       "      dtype='int64', length=600)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTA1cJRcQfWC",
    "outputId": "b9fca4ad-27bb-4201-9f39-76453be9bcef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([ 1151,  9111,  3326, 10891, 11322,  3709,  5261, 10060, 12446,  3089,\n",
       "       ...\n",
       "       13324, 11935,  9346, 11781,  3022, 10759, 11149,  7523, 12754,   667],\n",
       "      dtype='int64', length=200)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_class_1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2XoiZ5DQr7R",
    "outputId": "a89c28a2-2591-4395-f7f9-951de73216b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zp5fluP-QuBr",
    "outputId": "4c48e3df-4d44-433f-a92e-0e1931841fc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14040, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows of test_df and remaining_df when summed provides the number of rows of df\n",
    "remaining_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERSAMPLE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "hoyZUW4Ir8T2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampled DataFrame:\n",
      "airline_sentiment\n",
      "negative    8978\n",
      "neutral     8978\n",
      "positive    8978\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Using resample function from sklearn.utils for oversampling\n",
    "# Refer https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html for official documentation\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Checking existance of class imbalance in remaining_df\n",
    "# If there is class imbalance, we will oversample the data\n",
    "# i.e. we will make sure that number of rows for each class = number of rows for that class with maximum number of datapoints\n",
    "class_counts = remaining_df['airline_sentiment'].value_counts()\n",
    "max_class_count = class_counts.max()  # Get the number of samples in the majority class (class with highest number of datapoints)\n",
    "\n",
    "# Separate each class into three different DataFrames for negative, neutral and positive respectively\n",
    "negative_df = remaining_df[remaining_df['airline_sentiment'] == 'negative']\n",
    "neutral_df = remaining_df[remaining_df['airline_sentiment'] == 'neutral']\n",
    "positive_df = remaining_df[remaining_df['airline_sentiment'] == 'positive']\n",
    "\n",
    "# Oversampling the minority classes to match the number of samples in the majority class\n",
    "# We are oversampling the negative_df and neutral_df as they are the minority classes\n",
    "negative_df_oversampled = resample(negative_df,\n",
    "                                    replace=False,     # sample without replacement\n",
    "                                    n_samples=max_class_count,    # to match the number of majority samples\n",
    "                                    random_state=42)  # reproducible results with a fixed random_state\n",
    "\n",
    "neutral_df_oversampled = resample(neutral_df,\n",
    "                                   replace=False,     # sample without replacement\n",
    "                                   n_samples=max_class_count,    # to match the number of majority samples\n",
    "                                   random_state=42)  # reproducible results with a fixed random_state\n",
    "\n",
    "\n",
    "# negative_df_oversampled, neutral_df_oversampled and positive_df constitute the oversampled dataframes\n",
    "# Let's concatencate them to form our complete oversampled_df\n",
    "oversampled_df = pd.concat([negative_df, neutral_df_oversampled, positive_df_oversampled])\n",
    "\n",
    "# Let's shuffle the oversampled_df dataFrame so that the order of elements does not affect any sampling choices or training process\n",
    "# Also making sure the index is reset when shuffling\n",
    "# Refer to https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html\n",
    "oversampled_df = oversampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display the value counts to verify the balancing\n",
    "print(\"Oversampled DataFrame:\")\n",
    "print(oversampled_df['airline_sentiment'].value_counts())\n",
    "# 8978 datapoints per class (note that it matches with the length of the class with the lowest number of datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizing the training data (which is the oversampled data)\n",
    "train_df= oversampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When saving the weight, we are providing a prefix for the name, for ease of locating weights from this experiment\n",
    "weights_path_prefix = 'Glove_OVERSAMPLED_DATA_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jAKtnap6b2u"
   },
   "source": [
    "### LSTM GLOVE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINDATA TOKENIZATION AND PADDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6VEAyA-W8BMZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 821 ms, sys: 62.5 ms, total: 883 ms\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Let's create a tokenizer with 10000 words (all those words which do not come under this 10000, will be marked <OOV> )\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "# Train data preprocessing\n",
    "# Fitting the tokenizer on the traindata (only the text column)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "\n",
    "# Data pre-processing\n",
    "max_length = 200  # Maximum sequence length= 200\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'  # padding ensures embeddings of shorter sentences have the same length as that of the longer ones\n",
    "X_train = tokenizer.texts_to_sequences(train_df['text'])  # using the tokenizer on train_df['text']\n",
    "X_train = pad_sequences(X_train, maxlen=max_length, padding=padding_type, truncating=trunc_type)  # let's truncate and pad the tokens\n",
    "\n",
    "\n",
    "# Let's encode the labels\n",
    "y_train = label_encoder.fit_transform(train_df['airline_sentiment'])\n",
    "num_classes = len(label_encoder.classes_)   # number of classes\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)   # converting the labels to categorical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESTDATA TOKENIZATION AND PADDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sQyqOj6E8Oey"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.4 ms, sys: 2.9 ms, total: 15.3 ms\n",
      "Wall time: 21.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test data preprocessing\n",
    "# Performing the same steps of tokenizing,padding, and truncating on the test data too\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLASS WEIGHTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': 1.0, 'neutral': 1.0, 'positive': 1.0}\n",
      "{0: 1.0, 1: 1.0, 2: 1.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Find unique classes in train_df\n",
    "unique_classes = np.unique(train_df['airline_sentiment'])\n",
    "class_weights = {}\n",
    "# Calculate the weight for each class so that this weightage can be provided during training\n",
    "for cls in unique_classes:\n",
    "    class_weight = len(train_df['airline_sentiment']) / (len(unique_classes) * np.sum(train_df['airline_sentiment'] == cls))\n",
    "    class_weights[cls] = class_weight\n",
    "\n",
    "print(class_weights)\n",
    "# {0: 1.0, 1: 1.0, 2: 1.0} class weights are because all have equal weightage since class sizes are same for all 3 labels\n",
    "class_numbers= [0, 1, 2]\n",
    "class_weights= dict(zip(class_numbers,list(class_weights.values()))) \n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GLOVE EMBEDDINGS and MODEL CREATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_7WE8cn82Yd",
    "outputId": "f53bebf0-5ae0-4ff9-fdb6-6e0d2dc44251"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.15 s, sys: 1.24 s, total: 8.4 s\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load GloVe embeddings\n",
    "glove_path = '/Users/unnimaya/Documents/Projectexperiments/AIRLINEDATASET/glove.6B.100d.txt'\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_dim = 100  # Since we loaded 100 dimensional Glove mbeddings\n",
    "word_index = tokenizer.word_index   # Loading word index of Glove embeddings\n",
    "num_words = min(10000, len(word_index) + 1)   # Number of words same as that of our tokenizer we defined above\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))   # Each word has 100D, so size of embedding matrix will be 10000x100\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < 10000:    # if i<10000, assign embedding vector to the word from Glove\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector   # Embedding for the word found in Glove, value of that index of embedding_matrix will be embedding_vector\n",
    "\n",
    "\n",
    "# Build Bidirectional LSTM model with pre-trained Glove embeddings\n",
    "model = Sequential()\n",
    "# Create embedding for the input sequence using this first layer\n",
    "model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=max_length,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "# Using a series of Bidirectional LSTMs (with recurrent dropout) followed by dropout\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, recurrent_dropout=0.2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, recurrent_dropout=0.2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(128, recurrent_dropout=0.2)))\n",
    "model.add(Dropout(0.5))\n",
    "# After the LSTM and dropout layers, we have two fully connected neural network layers\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))  # number of neurons in output layer= number of classes, activation function is softmax for multiclass classification problems\n",
    "\n",
    "# Using learning rate 0.001\n",
    "learning_rate = 0.001\n",
    "# Using Adam optimizer\n",
    "optimizer = Adam(learning_rate=learning_rate, clipnorm=1.0) # using clipnorm to prevent exploding gradients if any\n",
    "\n",
    "# Compile the model with metric as accuracy and loss as categorical crossentropy (multiclass classification)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint callback- decides when to save weights\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath= weights_path_prefix+'_lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_{epoch:02d}.h5',  # Filepath format to include epoch number\n",
    "    save_weights_only=True,                         # Save only the model's weights\n",
    "    #save_freq=10 * (len(X_train) // 32),           # Save every 30 epochs (commented for now)\n",
    "    save_freq= 'epoch',                             # Save every epoch\n",
    "    verbose=1                                       # Print a message when saving the weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbqXpnTT882P",
    "outputId": "90bd905f-94fc-4439-c691-024839a4d2cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.7326 - accuracy: 0.6870\n",
      "Epoch 1: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_01.h5\n",
      "842/842 [==============================] - 3327s 4s/step - loss: 0.7326 - accuracy: 0.6870 - val_loss: 0.5862 - val_accuracy: 0.7700\n",
      "Epoch 2/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.5768 - accuracy: 0.7658\n",
      "Epoch 2: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_02.h5\n",
      "842/842 [==============================] - 3081s 4s/step - loss: 0.5768 - accuracy: 0.7658 - val_loss: 0.6013 - val_accuracy: 0.7850\n",
      "Epoch 3/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.4937 - accuracy: 0.8072\n",
      "Epoch 3: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_03.h5\n",
      "842/842 [==============================] - 2970s 4s/step - loss: 0.4937 - accuracy: 0.8072 - val_loss: 0.5417 - val_accuracy: 0.7783\n",
      "Epoch 4/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.4059 - accuracy: 0.8473\n",
      "Epoch 4: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_04.h5\n",
      "842/842 [==============================] - 2865s 3s/step - loss: 0.4059 - accuracy: 0.8473 - val_loss: 0.5977 - val_accuracy: 0.7750\n",
      "Epoch 5/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.3323 - accuracy: 0.8786\n",
      "Epoch 5: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_05.h5\n",
      "842/842 [==============================] - 2818s 3s/step - loss: 0.3323 - accuracy: 0.8786 - val_loss: 0.7549 - val_accuracy: 0.7667\n",
      "Epoch 6/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.2999 - accuracy: 0.8918\n",
      "Epoch 6: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_06.h5\n",
      "842/842 [==============================] - 2888s 3s/step - loss: 0.2999 - accuracy: 0.8918 - val_loss: 0.6815 - val_accuracy: 0.8117\n",
      "Epoch 7/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.2249 - accuracy: 0.9200\n",
      "Epoch 7: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_07.h5\n",
      "842/842 [==============================] - 2856s 3s/step - loss: 0.2249 - accuracy: 0.9200 - val_loss: 0.9729 - val_accuracy: 0.7433\n",
      "Epoch 8/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.1913 - accuracy: 0.9352\n",
      "Epoch 8: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_08.h5\n",
      "842/842 [==============================] - 2073s 2s/step - loss: 0.1913 - accuracy: 0.9352 - val_loss: 0.8397 - val_accuracy: 0.7850\n",
      "Epoch 9/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.1614 - accuracy: 0.9448\n",
      "Epoch 9: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_09.h5\n",
      "842/842 [==============================] - 1820s 2s/step - loss: 0.1614 - accuracy: 0.9448 - val_loss: 0.8772 - val_accuracy: 0.7883\n",
      "Epoch 10/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.1371 - accuracy: 0.9533\n",
      "Epoch 10: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_10.h5\n",
      "842/842 [==============================] - 1736s 2s/step - loss: 0.1371 - accuracy: 0.9533 - val_loss: 1.0807 - val_accuracy: 0.7683\n",
      "Epoch 11/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.1219 - accuracy: 0.9584\n",
      "Epoch 11: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_11.h5\n",
      "842/842 [==============================] - 1774s 2s/step - loss: 0.1219 - accuracy: 0.9584 - val_loss: 1.0168 - val_accuracy: 0.7683\n",
      "Epoch 12/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.1074 - accuracy: 0.9635\n",
      "Epoch 12: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_12.h5\n",
      "842/842 [==============================] - 1782s 2s/step - loss: 0.1074 - accuracy: 0.9635 - val_loss: 1.1171 - val_accuracy: 0.7600\n",
      "Epoch 13/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0960 - accuracy: 0.9680\n",
      "Epoch 13: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_13.h5\n",
      "842/842 [==============================] - 1789s 2s/step - loss: 0.0960 - accuracy: 0.9680 - val_loss: 0.9962 - val_accuracy: 0.7817\n",
      "Epoch 14/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0844 - accuracy: 0.9720\n",
      "Epoch 14: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_14.h5\n",
      "842/842 [==============================] - 1784s 2s/step - loss: 0.0844 - accuracy: 0.9720 - val_loss: 1.2386 - val_accuracy: 0.7600\n",
      "Epoch 15/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.9724\n",
      "Epoch 15: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_15.h5\n",
      "842/842 [==============================] - 1812s 2s/step - loss: 0.0823 - accuracy: 0.9724 - val_loss: 1.2221 - val_accuracy: 0.7583\n",
      "Epoch 16/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9758\n",
      "Epoch 16: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_16.h5\n",
      "842/842 [==============================] - 1579s 2s/step - loss: 0.0759 - accuracy: 0.9758 - val_loss: 1.1096 - val_accuracy: 0.7483\n",
      "Epoch 17/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9776\n",
      "Epoch 17: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_17.h5\n",
      "842/842 [==============================] - 1396s 2s/step - loss: 0.0678 - accuracy: 0.9776 - val_loss: 1.0678 - val_accuracy: 0.7617\n",
      "Epoch 18/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9794\n",
      "Epoch 18: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_18.h5\n",
      "842/842 [==============================] - 1377s 2s/step - loss: 0.0604 - accuracy: 0.9794 - val_loss: 1.2300 - val_accuracy: 0.7700\n",
      "Epoch 19/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9806\n",
      "Epoch 19: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_19.h5\n",
      "842/842 [==============================] - 1334s 2s/step - loss: 0.0575 - accuracy: 0.9806 - val_loss: 1.3372 - val_accuracy: 0.7583\n",
      "Epoch 20/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9808\n",
      "Epoch 20: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_20.h5\n",
      "842/842 [==============================] - 1338s 2s/step - loss: 0.0565 - accuracy: 0.9808 - val_loss: 1.4739 - val_accuracy: 0.7567\n",
      "Epoch 21/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9822\n",
      "Epoch 21: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_21.h5\n",
      "842/842 [==============================] - 1299s 2s/step - loss: 0.0535 - accuracy: 0.9822 - val_loss: 1.2837 - val_accuracy: 0.7367\n",
      "Epoch 22/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9820\n",
      "Epoch 22: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_22.h5\n",
      "842/842 [==============================] - 1350s 2s/step - loss: 0.0536 - accuracy: 0.9820 - val_loss: 1.2587 - val_accuracy: 0.7683\n",
      "Epoch 23/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9844\n",
      "Epoch 23: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_23.h5\n",
      "842/842 [==============================] - 1320s 2s/step - loss: 0.0474 - accuracy: 0.9844 - val_loss: 1.6503 - val_accuracy: 0.7333\n",
      "Epoch 24/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9838\n",
      "Epoch 24: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_24.h5\n",
      "842/842 [==============================] - 1512s 2s/step - loss: 0.0482 - accuracy: 0.9838 - val_loss: 1.5749 - val_accuracy: 0.7400\n",
      "Epoch 25/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9847\n",
      "Epoch 25: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_25.h5\n",
      "842/842 [==============================] - 1320s 2s/step - loss: 0.0465 - accuracy: 0.9847 - val_loss: 1.5917 - val_accuracy: 0.7517\n",
      "Epoch 26/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9846\n",
      "Epoch 26: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_26.h5\n",
      "842/842 [==============================] - 1337s 2s/step - loss: 0.0436 - accuracy: 0.9846 - val_loss: 1.7347 - val_accuracy: 0.7533\n",
      "Epoch 27/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9857\n",
      "Epoch 27: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_27.h5\n",
      "842/842 [==============================] - 1617s 2s/step - loss: 0.0454 - accuracy: 0.9857 - val_loss: 1.3237 - val_accuracy: 0.7667\n",
      "Epoch 28/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9860\n",
      "Epoch 28: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_28.h5\n",
      "842/842 [==============================] - 1987s 2s/step - loss: 0.0405 - accuracy: 0.9860 - val_loss: 1.8842 - val_accuracy: 0.7317\n",
      "Epoch 29/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9868\n",
      "Epoch 29: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_29.h5\n",
      "842/842 [==============================] - 1942s 2s/step - loss: 0.0385 - accuracy: 0.9868 - val_loss: 1.6324 - val_accuracy: 0.7533\n",
      "Epoch 30/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9876\n",
      "Epoch 30: saving model to Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_30.h5\n",
      "842/842 [==============================] - 1692s 2s/step - loss: 0.0373 - accuracy: 0.9876 - val_loss: 1.6443 - val_accuracy: 0.7417\n",
      "CPU times: user 17h 7min 58s, sys: 7h 25min 43s, total: 1d 33min 41s\n",
      "Wall time: 16h 2min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TRAINING THE MODEL\n",
    "# We supply test data as validation data to observe the model performance after each epoch\n",
    "# 30 iterations (epochs)\n",
    "# Uses class weight balancing based on class imbalance in data (no class imbalance here for oversampling)\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test), batch_size=32, callbacks=[checkpoint], class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and compile a saved weight\n",
    "model.load_weights('Glove_OVERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_08.h5')\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wmYpAQLx9FJt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 7s 375ms/step - loss: 0.8397 - accuracy: 0.7850\n",
      "Test Accuracy: 0.7850000262260437\n",
      "CPU times: user 8.61 s, sys: 3.35 s, total: 12 s\n",
      "Wall time: 7.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use the loaded weights and evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visulaization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_test is your test data\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)  # Get the predicted class labels\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming conf_matrix is the confusion matrix\n",
    "TP = np.diag(conf_matrix)\n",
    "FP = np.sum(conf_matrix, axis=0) - TP\n",
    "FN = np.sum(conf_matrix, axis=1) - TP\n",
    "\n",
    "# Avoid division by zero\n",
    "precision = np.divide(TP, (TP + FP), where=(TP + FP) != 0)\n",
    "recall = np.divide(TP, (TP + FN), where=(TP + FN) != 0)\n",
    "\n",
    "# Print precision and recall for each class\n",
    "for i in range(len(precision)):\n",
    "    print(f\"Class {i}: Precision = {precision[i]:.4f}, Recall = {recall[i]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
