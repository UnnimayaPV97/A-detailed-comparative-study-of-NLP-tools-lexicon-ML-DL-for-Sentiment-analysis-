{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the required packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PGmRwBRLt1Zv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/justin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/Users/justin/Library/Python/3.8/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Importing packages for pre-processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Packages for embeddings\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#Importing packages for cleaning the data\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Importing packages for deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Importing packages for visualization of results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydksZwWAj9Kt",
    "outputId": "f77bda3a-b115-4407-e89a-a7e76f699cab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to dataset\n",
    "dataset_path = '/Users/unnimaya/Documents/Projectexperiments/AIRLINEDATASET/usairlinetweets.csv'\n",
    "\n",
    "# Load the dataset in the path using pandas\n",
    "df_airline = pd.read_csv(dataset_path)\n",
    "\n",
    "# Display the initial rows of the dataframe\n",
    "df_airline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "B31SQQSikNuG",
    "outputId": "ccca8f18-918e-4fc2-c7e4-93b9131d1693"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_airline[[\"airline_sentiment\", \"text\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "GPT-iVxBksLb",
    "outputId": "0d4e7102-6664-47d4-d51f-59bb4d7f63be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment\n",
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"airline_sentiment\"].value_counts()\n",
    "#imbalanced data set\n",
    "#can use resampling techniques like k fold cross validation for better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the data\n",
    "lemmatizer = WordNetLemmatizer()#Initialize lemmatizer\n",
    "stop_words = set(stopwords.words('english'))#Initialize stopwords\n",
    "\n",
    "def text_cleaning(text):\n",
    "    text = text.lower()#Converting the text to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)#Punctuation removal using regular expression\n",
    "    text = re.sub(r'\\d+', '', text)#Number removal\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]#Stop word removal and lemmatization\n",
    "    cleaned_text = ' '.join(tokens)#Joins the tokens to form cleaned sentence   \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST DATA CREATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AuV1f6g2DOyQ",
    "outputId": "4057a000-8033-4709-aaef-0a8a7da900fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set class distribution:\n",
      " airline_sentiment\n",
      "negative    200\n",
      "neutral     200\n",
      "positive    200\n",
      "Name: count, dtype: int64\n",
      "Train set class distribution:\n",
      " airline_sentiment\n",
      "negative    8978\n",
      "neutral     2899\n",
      "positive    2163\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# The column 'airline_sentiment' is label\n",
    "class_label = 'airline_sentiment'\n",
    "\n",
    "# Sample 200 datapoints from each class for the test set\n",
    "test_df_class_1 = df[df[class_label] == 'negative'].sample(n=200, random_state=42)  # for class negative\n",
    "test_df_class_2 = df[df[class_label] == 'neutral'].sample(n=200, random_state=42)   # for class neutral\n",
    "test_df_class_3 = df[df[class_label] == 'positive'].sample(n=200, random_state=42)  # for class positive\n",
    "\n",
    "# Concatenate test_df_class_1, test_df_class_2 and test_df_class_3 to create the test set of 600 datapoints\n",
    "test_df = pd.concat([test_df_class_1, test_df_class_2, test_df_class_3])\n",
    "\n",
    "# original dataframe minus test_data = remaining data\n",
    "remaining_df = df.drop(test_df.index)\n",
    "\n",
    "# Display the number of samples in each class for both train and test sets\n",
    "# Check if the number of datapoints in remaining_df + test_df equals length of original dataframe\n",
    "print(\"Test set class distribution:\\n\", test_df[class_label].value_counts())\n",
    "print(\"Train set class distribution:\\n\", remaining_df[class_label].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FRjT5W4ScfI5"
   },
   "outputs": [],
   "source": [
    "# within test_df, \"text\" column is X and \"airline_sentiment\" column is Y\n",
    "# These will be referred to as X_test and y_test\n",
    "X_test= test_df[\"text\"]\n",
    "y_test= test_df[\"airline_sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVpEjzFXQYk_",
    "outputId": "a2635892-a971-4e82-96f4-ce099fd06039"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([ 1151,  9111,  3326, 10891, 11322,  3709,  5261, 10060, 12446,  3089,\n",
       "       ...\n",
       "       11705, 13422,  2716,  3071, 13504,  4489,  8271, 11795,  7294,  8203],\n",
       "      dtype='int64', length=600)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTA1cJRcQfWC",
    "outputId": "b9fca4ad-27bb-4201-9f39-76453be9bcef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([ 1151,  9111,  3326, 10891, 11322,  3709,  5261, 10060, 12446,  3089,\n",
       "       ...\n",
       "       13324, 11935,  9346, 11781,  3022, 10759, 11149,  7523, 12754,   667],\n",
       "      dtype='int64', length=200)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_class_1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2XoiZ5DQr7R",
    "outputId": "a89c28a2-2591-4395-f7f9-951de73216b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zp5fluP-QuBr",
    "outputId": "4c48e3df-4d44-433f-a92e-0e1931841fc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14040, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows of test_df and remaining_df when summed provides the number of rows of df\n",
    "remaining_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UNDERSAMPLE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undersampled DataFrame:\n",
      "airline_sentiment\n",
      "neutral     2163\n",
      "positive    2163\n",
      "negative    2163\n",
      "Name: count, dtype: int64\n",
      "CPU times: user 16.1 ms, sys: 2.38 ms, total: 18.5 ms\n",
      "Wall time: 16.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Using resample function from sklearn.utils for undersampling\n",
    "# Refer https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html for official documentation\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Checking existance of class imbalance in remaining_df\n",
    "# If there is class imbalance, we will undersample the data\n",
    "# i.e. we will make sure that number of rows for each class = number of rows for that class with lowest number of datapoints\n",
    "class_counts = remaining_df['airline_sentiment'].value_counts()\n",
    "min_class_count = class_counts.min()  # Get the number of samples in the minority class (class with lowest number of datapoints)\n",
    "\n",
    "# Separate each class into three different DataFrames for negative, neutral and positive respectively\n",
    "negative_df = remaining_df[remaining_df['airline_sentiment'] == 'negative']\n",
    "neutral_df = remaining_df[remaining_df['airline_sentiment'] == 'neutral']\n",
    "positive_df = remaining_df[remaining_df['airline_sentiment'] == 'positive']\n",
    "\n",
    "# Undersampling the majority classes to match the number of samples in the minority class\n",
    "# We are undersampling only the negative_df and neutral_df as they are the non-minority classes\n",
    "negative_df_undersampled = resample(negative_df,\n",
    "                                    replace=False,     # sample without replacement\n",
    "                                    n_samples=min_class_count,    # to match the number of minority samples\n",
    "                                    random_state=42)  # reproducible results with a fixed random_state\n",
    "\n",
    "neutral_df_undersampled = resample(neutral_df,\n",
    "                                   replace=False,     # sample without replacement\n",
    "                                   n_samples=min_class_count,    # to match the number of minority samples\n",
    "                                   random_state=42)  # reproducible results with a fixed random_state\n",
    "\n",
    "# negative_df_undersampled, neutral_df_undersampled and positive_df constitute the undersampled dataframes\n",
    "# Let's concatencate them to form our complete undersampled_df\n",
    "undersampled_df = pd.concat([negative_df_undersampled, neutral_df_undersampled, positive_df])\n",
    "\n",
    "# Let's shuffle the undersampled_df dataFrame so that the order of elements does not affect any sampling choices or training process\n",
    "# Also making sure the index is reset when shuffling\n",
    "# Refer to https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html\n",
    "undersampled_df = undersampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display the value counts to verify the balancing\n",
    "print(\"Undersampled DataFrame:\")\n",
    "print(undersampled_df['airline_sentiment'].value_counts()) # 2163 datapoints per class (note that it matches with the length of the class with the lowest number of datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizing the training data (which is the undersampled data)\n",
    "train_df= undersampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When saving the weight, we are providing a prefix for the name, for ease of locating weights from this experiment\n",
    "weights_path_prefix = 'Word2vec_UNDERSAMPLED_DATA_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jAKtnap6b2u"
   },
   "source": [
    "### LSTM WORD2VEC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINDATA TOKENIZATION AND PADDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6VEAyA-W8BMZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 164 ms, sys: 4.38 ms, total: 169 ms\n",
      "Wall time: 168 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Let's create a tokenizer with 10000 words (all those words which do not come under this 10000, will be marked <OOV> )\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "# Train data preprocessing\n",
    "# Fitting the tokenizer on the traindata (only the text column)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "\n",
    "# Data pre-processing\n",
    "max_length = 200  # Maximum sequence length= 200\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'  # padding ensures embeddings of shorter sentences have the same length as that of the longer ones\n",
    "X_train = tokenizer.texts_to_sequences(train_df['text'])  # using the tokenizer on train_df['text']\n",
    "X_train = pad_sequences(X_train, maxlen=max_length, padding=padding_type, truncating=trunc_type)  # let's truncate and pad the tokens\n",
    "\n",
    "\n",
    "# Let's encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_df['airline_sentiment'])\n",
    "num_classes = len(label_encoder.classes_)   # number of classes\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)   # converting the labels to categorical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESTDATA TOKENIZATION AND PADDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "sQyqOj6E8Oey"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 ms, sys: 4.4 ms, total: 27.4 ms\n",
      "Wall time: 27 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test data preprocessing\n",
    "# Performing the same steps of tokenizing,padding, and truncating on the test data too\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLASS WEIGHTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': 1.0, 'neutral': 1.0, 'positive': 1.0}\n",
      "{0: 1.0, 1: 1.0, 2: 1.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Find unique classes in train_df\n",
    "unique_classes = np.unique(train_df['airline_sentiment'])\n",
    "class_weights = {}\n",
    "# Calculate the weight for each class so that this weightage can be provided during training\n",
    "for cls in unique_classes:\n",
    "    class_weight = len(train_df['airline_sentiment']) / (len(unique_classes) * np.sum(train_df['airline_sentiment'] == cls))\n",
    "    class_weights[cls] = class_weight\n",
    "\n",
    "print(class_weights)\n",
    "# {0: 1.0, 1: 1.0, 2: 1.0} class weights are because all have equal weightage since class sizes are same for all 3 labels\n",
    "class_numbers= [0, 1, 2]\n",
    "class_weights= dict(zip(class_numbers,list(class_weights.values()))) \n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WORD2VEC EMBEDDINGS and MODEL CREATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_7WE8cn82Yd",
    "outputId": "f53bebf0-5ae0-4ff9-fdb6-6e0d2dc44251"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.5 s, sys: 1.23 s, total: 12.7 s\n",
      "Wall time: 37.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load Word2Vec embeddings\n",
    "word2vec_path = '/Users/unnimaya/Documents/Projectexperiments/AIRLINEDATASET/GoogleNews-vectors-negative300.bin'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "\n",
    "embedding_dim = 300  # Since we loaded 300 dimensional Word2Vec mbeddings\n",
    "word_index = tokenizer.word_index  # Loading word index of Word2Vec embeddings\n",
    "num_words = min(10000, len(word_index) + 1)   # Number of words same as that of our tokenizer we defined above\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))    # Each word has 300D, so size of embedding matrix will be 10000x300\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < 10000:  # if i<10000, assign embedding vector to the word from Word2Vec\n",
    "        try:\n",
    "            embedding_vector = word2vec[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            pass  # Word not found in Word2Vec, the embedding will be np.zeros((num_words, embedding_dim))\n",
    "            \n",
    "# Build Bidirectional LSTM model with pre-trained Word2Vec embeddings\n",
    "model = Sequential()\n",
    "# Create embedding for the input sequence using this first layer\n",
    "model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=max_length,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "# Using a series of Bidirectional LSTMs (with recurrent dropout) followed by dropout\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, recurrent_dropout=0.2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, recurrent_dropout=0.2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(128, recurrent_dropout=0.2)))\n",
    "model.add(Dropout(0.5))\n",
    "# After the LSTM and dropout layers, we have two fully connected neural network layers\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))  # number of neurons in output layer= number of classes, activation function is softmax for multiclass classification problems\n",
    "\n",
    "# Using learning rate 0.01\n",
    "learning_rate = 0.001\n",
    "# Using Adam optimizer\n",
    "optimizer = Adam(learning_rate=learning_rate, clipnorm=1.0) # using clipnorm to prevent exploding gradients if any\n",
    "\n",
    "# Compile the model with metric as accuracy and loss as categorical crossentropy (multiclass classification)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint callback- decides when to save weights\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath= weights_path_prefix+'_lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_{epoch:02d}.h5',  # Filepath format to include epoch number\n",
    "    save_weights_only=True,                         # Save only the model's weights\n",
    "    #save_freq=10 * (len(X_train) // 32),           # Save every 30 epochs (commented for now)\n",
    "    save_freq= 'epoch',                             # Save every epoch\n",
    "    verbose=1                                       # Print a message when saving the weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbqXpnTT882P",
    "outputId": "90bd905f-94fc-4439-c691-024839a4d2cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.8005 - accuracy: 0.6577\n",
      "Epoch 1: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_01.h5\n",
      "203/203 [==============================] - 780s 4s/step - loss: 0.8005 - accuracy: 0.6577 - val_loss: 0.6378 - val_accuracy: 0.7350\n",
      "Epoch 2/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.6435 - accuracy: 0.7396\n",
      "Epoch 2: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_02.h5\n",
      "203/203 [==============================] - 852s 4s/step - loss: 0.6435 - accuracy: 0.7396 - val_loss: 0.6018 - val_accuracy: 0.7700\n",
      "Epoch 3/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.6055 - accuracy: 0.7602\n",
      "Epoch 3: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_03.h5\n",
      "203/203 [==============================] - 704s 3s/step - loss: 0.6055 - accuracy: 0.7602 - val_loss: 0.5614 - val_accuracy: 0.7917\n",
      "Epoch 4/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.5823 - accuracy: 0.7725\n",
      "Epoch 4: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_04.h5\n",
      "203/203 [==============================] - 734s 4s/step - loss: 0.5823 - accuracy: 0.7725 - val_loss: 0.5753 - val_accuracy: 0.7667\n",
      "Epoch 5/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.5487 - accuracy: 0.7830\n",
      "Epoch 5: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_05.h5\n",
      "203/203 [==============================] - 653s 3s/step - loss: 0.5487 - accuracy: 0.7830 - val_loss: 0.5546 - val_accuracy: 0.7967\n",
      "Epoch 6/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.5254 - accuracy: 0.7944\n",
      "Epoch 6: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_06.h5\n",
      "203/203 [==============================] - 675s 3s/step - loss: 0.5254 - accuracy: 0.7944 - val_loss: 0.5377 - val_accuracy: 0.7783\n",
      "Epoch 7/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.4868 - accuracy: 0.8128\n",
      "Epoch 7: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_07.h5\n",
      "203/203 [==============================] - 666s 3s/step - loss: 0.4868 - accuracy: 0.8128 - val_loss: 0.5418 - val_accuracy: 0.7867\n",
      "Epoch 8/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.4660 - accuracy: 0.8191\n",
      "Epoch 8: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_08.h5\n",
      "203/203 [==============================] - 635s 3s/step - loss: 0.4660 - accuracy: 0.8191 - val_loss: 0.5700 - val_accuracy: 0.7750\n",
      "Epoch 9/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.4289 - accuracy: 0.8382\n",
      "Epoch 9: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_09.h5\n",
      "203/203 [==============================] - 665s 3s/step - loss: 0.4289 - accuracy: 0.8382 - val_loss: 0.6535 - val_accuracy: 0.7667\n",
      "Epoch 10/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.4041 - accuracy: 0.8436\n",
      "Epoch 10: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_10.h5\n",
      "203/203 [==============================] - 667s 3s/step - loss: 0.4041 - accuracy: 0.8436 - val_loss: 0.5697 - val_accuracy: 0.7833\n",
      "Epoch 11/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.3691 - accuracy: 0.8590\n",
      "Epoch 11: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_11.h5\n",
      "203/203 [==============================] - 679s 3s/step - loss: 0.3691 - accuracy: 0.8590 - val_loss: 0.6466 - val_accuracy: 0.7700\n",
      "Epoch 12/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.3378 - accuracy: 0.8732\n",
      "Epoch 12: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_12.h5\n",
      "203/203 [==============================] - 655s 3s/step - loss: 0.3378 - accuracy: 0.8732 - val_loss: 0.6713 - val_accuracy: 0.7600\n",
      "Epoch 13/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.3133 - accuracy: 0.8863\n",
      "Epoch 13: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_13.h5\n",
      "203/203 [==============================] - 677s 3s/step - loss: 0.3133 - accuracy: 0.8863 - val_loss: 0.7014 - val_accuracy: 0.7700\n",
      "Epoch 14/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.2829 - accuracy: 0.8969\n",
      "Epoch 14: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_14.h5\n",
      "203/203 [==============================] - 647s 3s/step - loss: 0.2829 - accuracy: 0.8969 - val_loss: 0.6848 - val_accuracy: 0.7717\n",
      "Epoch 15/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.2427 - accuracy: 0.9109\n",
      "Epoch 15: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_15.h5\n",
      "203/203 [==============================] - 663s 3s/step - loss: 0.2427 - accuracy: 0.9109 - val_loss: 0.7213 - val_accuracy: 0.7967\n",
      "Epoch 16/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.2288 - accuracy: 0.9145\n",
      "Epoch 16: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_16.h5\n",
      "203/203 [==============================] - 670s 3s/step - loss: 0.2288 - accuracy: 0.9145 - val_loss: 0.8036 - val_accuracy: 0.7800\n",
      "Epoch 17/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.2163 - accuracy: 0.9253\n",
      "Epoch 17: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_17.h5\n",
      "203/203 [==============================] - 672s 3s/step - loss: 0.2163 - accuracy: 0.9253 - val_loss: 0.7908 - val_accuracy: 0.7917\n",
      "Epoch 18/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9320\n",
      "Epoch 18: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_18.h5\n",
      "203/203 [==============================] - 669s 3s/step - loss: 0.1927 - accuracy: 0.9320 - val_loss: 0.7078 - val_accuracy: 0.7717\n",
      "Epoch 19/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.1670 - accuracy: 0.9428\n",
      "Epoch 19: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_19.h5\n",
      "203/203 [==============================] - 692s 3s/step - loss: 0.1670 - accuracy: 0.9428 - val_loss: 0.8180 - val_accuracy: 0.7833\n",
      "Epoch 20/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.1521 - accuracy: 0.9465\n",
      "Epoch 20: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_20.h5\n",
      "203/203 [==============================] - 662s 3s/step - loss: 0.1521 - accuracy: 0.9465 - val_loss: 0.8885 - val_accuracy: 0.7867\n",
      "Epoch 21/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.1400 - accuracy: 0.9510\n",
      "Epoch 21: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_21.h5\n",
      "203/203 [==============================] - 681s 3s/step - loss: 0.1400 - accuracy: 0.9510 - val_loss: 0.9268 - val_accuracy: 0.7817\n",
      "Epoch 22/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.1192 - accuracy: 0.9601\n",
      "Epoch 22: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_22.h5\n",
      "203/203 [==============================] - 711s 4s/step - loss: 0.1192 - accuracy: 0.9601 - val_loss: 0.9558 - val_accuracy: 0.7817\n",
      "Epoch 23/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.9615\n",
      "Epoch 23: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_23.h5\n",
      "203/203 [==============================] - 676s 3s/step - loss: 0.1202 - accuracy: 0.9615 - val_loss: 1.0226 - val_accuracy: 0.7833\n",
      "Epoch 24/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.9564\n",
      "Epoch 24: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_24.h5\n",
      "203/203 [==============================] - 664s 3s/step - loss: 0.1239 - accuracy: 0.9564 - val_loss: 0.9283 - val_accuracy: 0.7817\n",
      "Epoch 25/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.0953 - accuracy: 0.9669\n",
      "Epoch 25: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_25.h5\n",
      "203/203 [==============================] - 673s 3s/step - loss: 0.0953 - accuracy: 0.9669 - val_loss: 1.0445 - val_accuracy: 0.7750\n",
      "Epoch 26/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9733\n",
      "Epoch 26: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_26.h5\n",
      "203/203 [==============================] - 683s 3s/step - loss: 0.0869 - accuracy: 0.9733 - val_loss: 1.1059 - val_accuracy: 0.7650\n",
      "Epoch 27/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.0888 - accuracy: 0.9713\n",
      "Epoch 27: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_27.h5\n",
      "203/203 [==============================] - 662s 3s/step - loss: 0.0888 - accuracy: 0.9713 - val_loss: 1.2159 - val_accuracy: 0.7533\n",
      "Epoch 28/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.0935 - accuracy: 0.9687\n",
      "Epoch 28: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_28.h5\n",
      "203/203 [==============================] - 689s 3s/step - loss: 0.0935 - accuracy: 0.9687 - val_loss: 1.0489 - val_accuracy: 0.7717\n",
      "Epoch 29/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.0840 - accuracy: 0.9720\n",
      "Epoch 29: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_29.h5\n",
      "203/203 [==============================] - 650s 3s/step - loss: 0.0840 - accuracy: 0.9720 - val_loss: 1.1034 - val_accuracy: 0.7967\n",
      "Epoch 30/30\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9753\n",
      "Epoch 30: saving model to Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_30.h5\n",
      "203/203 [==============================] - 658s 3s/step - loss: 0.0715 - accuracy: 0.9753 - val_loss: 1.2108 - val_accuracy: 0.7800\n",
      "CPU times: user 5h 59min 5s, sys: 2h 27min 57s, total: 8h 27min 3s\n",
      "Wall time: 5h 41min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TRAINING THE MODEL\n",
    "# We supply test data as validation data to observe the model performance after each epoch\n",
    "# 30 iterations (epochs)\n",
    "# Uses class weight balancing based on class imbalance in data (no class imbalance here for undersampling)\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test), batch_size=32, callbacks=[checkpoint], class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# load and compile a saved weight\n",
    "model.load_weights('Word2vec_UNDERSAMPLED_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_15.h5')\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "wmYpAQLx9FJt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 6s 268ms/step - loss: 0.7213 - accuracy: 0.7967\n",
      "Test Accuracy: 0.79666668176651\n",
      "CPU times: user 9.67 s, sys: 3.03 s, total: 12.7 s\n",
      "Wall time: 5.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use the loaded weights and evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VISUALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 6s 308ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's print a confusion matrix \n",
    "\n",
    "# Assuming X_test is your test data\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)  # Get the predicted class labels\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: Precision = 0.7980, Recall = 0.8100\n",
      "Class 1: Precision = 0.7363, Recall = 0.7400\n",
      "Class 2: Precision = 0.8571, Recall = 0.8400\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming conf_matrix is the confusion matrix\n",
    "TP = np.diag(conf_matrix)\n",
    "FP = np.sum(conf_matrix, axis=0) - TP\n",
    "FN = np.sum(conf_matrix, axis=1) - TP\n",
    "\n",
    "# Avoid division by zero\n",
    "precision = np.divide(TP, (TP + FP), where=(TP + FP) != 0)\n",
    "recall = np.divide(TP, (TP + FN), where=(TP + FN) != 0)\n",
    "\n",
    "# Print precision and recall for each class\n",
    "for i in range(len(precision)):\n",
    "    print(f\"Class {i}: Precision = {precision[i]:.4f}, Recall = {recall[i]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
