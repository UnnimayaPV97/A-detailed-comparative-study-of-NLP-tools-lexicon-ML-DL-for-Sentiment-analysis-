{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the required packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PGmRwBRLt1Zv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/unnimaya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing packages for pre-processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Importing packages for cleaning the data\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Packages for embeddings\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Importing packages for deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Importing packages for visualization of results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydksZwWAj9Kt",
    "outputId": "f77bda3a-b115-4407-e89a-a7e76f699cab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to dataset\n",
    "dataset_path = '/Users/unnimaya/Documents/Projectexperiments/AIRLINEDATASET/usairlinetweets.csv'\n",
    "\n",
    "# Load the dataset in the path using pandas\n",
    "df_airline = pd.read_csv(dataset_path)\n",
    "\n",
    "# Display the initial rows of the dataframe\n",
    "df_airline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "B31SQQSikNuG",
    "outputId": "ccca8f18-918e-4fc2-c7e4-93b9131d1693"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_airline[[\"airline_sentiment\", \"text\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "GPT-iVxBksLb",
    "outputId": "0d4e7102-6664-47d4-d51f-59bb4d7f63be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment\n",
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"airline_sentiment\"].value_counts()\n",
    "#imbalanced data set\n",
    "#can use resampling techniques like k fold cross validation for better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the data\n",
    "lemmatizer = WordNetLemmatizer()#Initialize lemmatizer\n",
    "stop_words = set(stopwords.words('english'))#Initialize stopwords\n",
    "\n",
    "def text_cleaning(text):\n",
    "    text = text.lower()#Converting the text to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)#Punctuation removal using regular expression\n",
    "    text = re.sub(r'\\d+', '', text)#Number removal\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]#Stop word removal and lemmatization\n",
    "    cleaned_text = ' '.join(tokens)#Joins the tokens to form cleaned sentence   \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST DATA CREATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AuV1f6g2DOyQ",
    "outputId": "4057a000-8033-4709-aaef-0a8a7da900fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set class distribution:\n",
      " airline_sentiment\n",
      "negative    200\n",
      "neutral     200\n",
      "positive    200\n",
      "Name: count, dtype: int64\n",
      "Train set class distribution:\n",
      " airline_sentiment\n",
      "negative    8978\n",
      "neutral     2899\n",
      "positive    2163\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# The column 'airline_sentiment' is label\n",
    "class_label = 'airline_sentiment'\n",
    "\n",
    "# Sample 200 datapoints from each class for the test set\n",
    "test_df_class_1 = df[df[class_label] == 'negative'].sample(n=200, random_state=42)  # for class negative\n",
    "test_df_class_2 = df[df[class_label] == 'neutral'].sample(n=200, random_state=42)   # for class neutral\n",
    "test_df_class_3 = df[df[class_label] == 'positive'].sample(n=200, random_state=42)  # for class positive\n",
    "\n",
    "# Concatenate test_df_class_1, test_df_class_2 and test_df_class_3 to create the test set of 600 datapoints\n",
    "test_df = pd.concat([test_df_class_1, test_df_class_2, test_df_class_3])\n",
    "\n",
    "# original dataframe minus test_data = remaining data\n",
    "remaining_df = df.drop(test_df.index)\n",
    "\n",
    "# Display the number of samples in each class for both train and test sets\n",
    "# Check if the number of datapoints in remaining_df + test_df equals length of original dataframe\n",
    "print(\"Test set class distribution:\\n\", test_df[class_label].value_counts())\n",
    "print(\"Train set class distribution:\\n\", remaining_df[class_label].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "FRjT5W4ScfI5"
   },
   "outputs": [],
   "source": [
    "# within test_df, \"text\" column is X and \"airline_sentiment\" column is Y\n",
    "# These will be referred to as X_test and y_test\n",
    "X_test= test_df[\"text\"]\n",
    "y_test= test_df[\"airline_sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVpEjzFXQYk_",
    "outputId": "a2635892-a971-4e82-96f4-ce099fd06039"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([ 1151,  9111,  3326, 10891, 11322,  3709,  5261, 10060, 12446,  3089,\n",
       "       ...\n",
       "       11705, 13422,  2716,  3071, 13504,  4489,  8271, 11795,  7294,  8203],\n",
       "      dtype='int64', length=600)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTA1cJRcQfWC",
    "outputId": "b9fca4ad-27bb-4201-9f39-76453be9bcef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([ 1151,  9111,  3326, 10891, 11322,  3709,  5261, 10060, 12446,  3089,\n",
       "       ...\n",
       "       13324, 11935,  9346, 11781,  3022, 10759, 11149,  7523, 12754,   667],\n",
       "      dtype='int64', length=200)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_class_1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2XoiZ5DQr7R",
    "outputId": "a89c28a2-2591-4395-f7f9-951de73216b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zp5fluP-QuBr",
    "outputId": "4c48e3df-4d44-433f-a92e-0e1931841fc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14040, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows of test_df and remaining_df when summed provides the number of rows of df\n",
    "remaining_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is no oversampling or undersampling involved here, so train_df= remaining_df\n",
    "train_df= remaining_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When saving the weight, we are providing a prefix for the name, for ease of locating weights from this experiment\n",
    "weights_path_prefix = 'Glove_ORIGINAL_DATA_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jAKtnap6b2u"
   },
   "source": [
    "### LSTM GLOVE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINDATA TOKENIZATION AND PADDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6VEAyA-W8BMZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 363 ms, sys: 84.1 ms, total: 447 ms\n",
      "Wall time: 596 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Let's create a tokenizer with 10000 words (all those words which do not come under this 10000, will be marked <OOV> )\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "# Train data preprocessing\n",
    "# Fitting the tokenizer on the traindata (only the text column)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "\n",
    "# Data pre-processing\n",
    "max_length = 200  # Maximum sequence length= 200\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'  # padding ensures embeddings of shorter sentences have the same length as that of the longer ones\n",
    "X_train = tokenizer.texts_to_sequences(train_df['text'])  # using the tokenizer on train_df['text']\n",
    "X_train = pad_sequences(X_train, maxlen=max_length, padding=padding_type, truncating=trunc_type)  # let's truncate and pad the tokens\n",
    "\n",
    "\n",
    "# Let's encode the labels\n",
    "y_train = label_encoder.fit_transform(train_df['airline_sentiment'])\n",
    "num_classes = len(label_encoder.classes_)   # number of classes\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)   # converting the labels to categorical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESTDATA TOKENIZATION AND PADDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "sQyqOj6E8Oey"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.65 ms, sys: 902 µs, total: 6.55 ms\n",
      "Wall time: 5.88 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test data preprocessing\n",
    "# Performing the same steps of tokenizing,padding, and truncating on the test data too\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLASS WEIGHTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': 0.5212742258854979, 'neutral': 1.6143497757847534, 'positive': 2.163661581137309}\n",
      "{0: 0.5212742258854979, 1: 1.6143497757847534, 2: 2.163661581137309}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Find unique classes in train_df\n",
    "unique_classes = np.unique(train_df['airline_sentiment'])\n",
    "class_weights = {}\n",
    "# Calculate the weight for each class so that this weightage can be provided during training\n",
    "for cls in unique_classes:\n",
    "    class_weight = len(train_df['airline_sentiment']) / (len(unique_classes) * np.sum(train_df['airline_sentiment'] == cls))\n",
    "    class_weights[cls] = class_weight\n",
    "\n",
    "print(class_weights)\n",
    "# {0: 0.5212742258854979, 1: 1.6143497757847534, 2: 2.163661581137309} class weights are because they have unequal weightage since class sizes are different for all 3 labels\n",
    "class_numbers= [0, 1, 2]\n",
    "class_weights= dict(zip(class_numbers,list(class_weights.values()))) \n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GLOVE EMBEDDINGS and MODEL CREATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_7WE8cn82Yd",
    "outputId": "f53bebf0-5ae0-4ff9-fdb6-6e0d2dc44251"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.4 s, sys: 337 ms, total: 4.74 s\n",
      "Wall time: 6.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load GloVe embeddings\n",
    "glove_path = '/Users/unnimaya/Documents/Projectexperiments/AIRLINEDATASET/glove.6B.100d.txt'\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_dim = 100    # Since we loaded 100 dimensional Glove mbeddings\n",
    "word_index = tokenizer.word_index    # Loading word index of Glove embeddings\n",
    "num_words = min(10000, len(word_index) + 1)    # Number of words same as that of our tokenizer we defined above\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))    # Each word has 100D, so size of embedding matrix will be 10000x100\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < 10000:    # if i<10000, assign embedding vector to the word from Glove\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "# Build Bidirectional LSTM model with pre-trained Word2Vec embeddings\n",
    "model = Sequential()\n",
    "# Create embedding for the input sequence using this first layer\n",
    "model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=max_length,\n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "# Using a series of Bidirectional LSTMs (with recurrent dropout) followed by dropout\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, recurrent_dropout=0.2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, recurrent_dropout=0.2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(128, recurrent_dropout=0.2)))\n",
    "model.add(Dropout(0.5))\n",
    "# After the LSTM and dropout layers, we have two fully connected neural network layers\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))  # number of neurons in output layer= number of classes, activation function is softmax for multiclass classification problems\n",
    "\n",
    "# Using learning rate 0.01\n",
    "learning_rate = 0.001\n",
    "# Using Adam optimizer\n",
    "optimizer = Adam(learning_rate=learning_rate, clipnorm=1.0) # using clipnorm to prevent exploding gradients if any\n",
    "\n",
    "# Compile the model with metric as accuracy and loss as categorical crossentropy (multiclass classification)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint callback- decides when to save weights\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath= weights_path_prefix+'_lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_{epoch:02d}.h5',  # Filepath format to include epoch number\n",
    "    save_weights_only=True,                         # Save only the model's weights\n",
    "    #save_freq=10 * (len(X_train) // 32),           # Save every 30 epochs (commented for now)\n",
    "    save_freq= 'epoch',                             # Save every epoch\n",
    "    verbose=1                                       # Print a message when saving the weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbqXpnTT882P",
    "outputId": "90bd905f-94fc-4439-c691-024839a4d2cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.8222 - accuracy: 0.6554\n",
      "Epoch 1: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_01.h5\n",
      "439/439 [==============================] - 1733s 4s/step - loss: 0.8222 - accuracy: 0.6554 - val_loss: 0.6557 - val_accuracy: 0.7267\n",
      "Epoch 2/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.6946 - accuracy: 0.7165\n",
      "Epoch 2: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_02.h5\n",
      "439/439 [==============================] - 1753s 4s/step - loss: 0.6946 - accuracy: 0.7165 - val_loss: 0.6071 - val_accuracy: 0.7517\n",
      "Epoch 3/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.6483 - accuracy: 0.7344\n",
      "Epoch 3: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_03.h5\n",
      "439/439 [==============================] - 1601s 4s/step - loss: 0.6483 - accuracy: 0.7344 - val_loss: 0.6211 - val_accuracy: 0.7283\n",
      "Epoch 4/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.6260 - accuracy: 0.7469\n",
      "Epoch 4: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_04.h5\n",
      "439/439 [==============================] - 1607s 4s/step - loss: 0.6260 - accuracy: 0.7469 - val_loss: 0.5512 - val_accuracy: 0.7750\n",
      "Epoch 5/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.5938 - accuracy: 0.7583\n",
      "Epoch 5: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_05.h5\n",
      "439/439 [==============================] - 1547s 4s/step - loss: 0.5938 - accuracy: 0.7583 - val_loss: 0.5612 - val_accuracy: 0.8117\n",
      "Epoch 6/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.5627 - accuracy: 0.7724\n",
      "Epoch 6: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_06.h5\n",
      "439/439 [==============================] - 1538s 4s/step - loss: 0.5627 - accuracy: 0.7724 - val_loss: 0.5357 - val_accuracy: 0.8000\n",
      "Epoch 7/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.5270 - accuracy: 0.7873\n",
      "Epoch 7: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_07.h5\n",
      "439/439 [==============================] - 1483s 3s/step - loss: 0.5270 - accuracy: 0.7873 - val_loss: 0.5082 - val_accuracy: 0.8033\n",
      "Epoch 8/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.4957 - accuracy: 0.7996\n",
      "Epoch 8: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_08.h5\n",
      "439/439 [==============================] - 1502s 3s/step - loss: 0.4957 - accuracy: 0.7996 - val_loss: 0.5736 - val_accuracy: 0.7850\n",
      "Epoch 9/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.4658 - accuracy: 0.8105\n",
      "Epoch 9: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_09.h5\n",
      "439/439 [==============================] - 1463s 3s/step - loss: 0.4658 - accuracy: 0.8105 - val_loss: 0.5280 - val_accuracy: 0.8083\n",
      "Epoch 10/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.4378 - accuracy: 0.8186\n",
      "Epoch 10: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_10.h5\n",
      "439/439 [==============================] - 1462s 3s/step - loss: 0.4378 - accuracy: 0.8186 - val_loss: 0.5362 - val_accuracy: 0.7967\n",
      "Epoch 11/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.3990 - accuracy: 0.8370\n",
      "Epoch 11: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_11.h5\n",
      "439/439 [==============================] - 1490s 3s/step - loss: 0.3990 - accuracy: 0.8370 - val_loss: 0.6644 - val_accuracy: 0.7633\n",
      "Epoch 12/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.3700 - accuracy: 0.8472\n",
      "Epoch 12: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_12.h5\n",
      "439/439 [==============================] - 1503s 3s/step - loss: 0.3700 - accuracy: 0.8472 - val_loss: 0.6677 - val_accuracy: 0.8000\n",
      "Epoch 13/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.3548 - accuracy: 0.8578\n",
      "Epoch 13: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_13.h5\n",
      "439/439 [==============================] - 1461s 3s/step - loss: 0.3548 - accuracy: 0.8578 - val_loss: 0.6589 - val_accuracy: 0.7833\n",
      "Epoch 14/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.3263 - accuracy: 0.8662\n",
      "Epoch 14: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_14.h5\n",
      "439/439 [==============================] - 1498s 3s/step - loss: 0.3263 - accuracy: 0.8662 - val_loss: 0.6657 - val_accuracy: 0.7867\n",
      "Epoch 15/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.3023 - accuracy: 0.8761\n",
      "Epoch 15: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_15.h5\n",
      "439/439 [==============================] - 976s 2s/step - loss: 0.3023 - accuracy: 0.8761 - val_loss: 0.6732 - val_accuracy: 0.7800\n",
      "Epoch 16/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.2693 - accuracy: 0.8918\n",
      "Epoch 16: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_16.h5\n",
      "439/439 [==============================] - 895s 2s/step - loss: 0.2693 - accuracy: 0.8918 - val_loss: 0.7577 - val_accuracy: 0.7767\n",
      "Epoch 17/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.2512 - accuracy: 0.8953\n",
      "Epoch 17: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_17.h5\n",
      "439/439 [==============================] - 949s 2s/step - loss: 0.2512 - accuracy: 0.8953 - val_loss: 0.7758 - val_accuracy: 0.7867\n",
      "Epoch 18/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.2264 - accuracy: 0.9066\n",
      "Epoch 18: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_18.h5\n",
      "439/439 [==============================] - 928s 2s/step - loss: 0.2264 - accuracy: 0.9066 - val_loss: 0.9459 - val_accuracy: 0.7767\n",
      "Epoch 19/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.2118 - accuracy: 0.9123\n",
      "Epoch 19: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_19.h5\n",
      "439/439 [==============================] - 956s 2s/step - loss: 0.2118 - accuracy: 0.9123 - val_loss: 0.8156 - val_accuracy: 0.7783\n",
      "Epoch 20/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.1990 - accuracy: 0.9165\n",
      "Epoch 20: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_20.h5\n",
      "439/439 [==============================] - 928s 2s/step - loss: 0.1990 - accuracy: 0.9165 - val_loss: 0.8547 - val_accuracy: 0.7917\n",
      "Epoch 21/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.1886 - accuracy: 0.9234\n",
      "Epoch 21: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_21.h5\n",
      "439/439 [==============================] - 988s 2s/step - loss: 0.1886 - accuracy: 0.9234 - val_loss: 0.8776 - val_accuracy: 0.7850\n",
      "Epoch 22/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.1662 - accuracy: 0.9318\n",
      "Epoch 22: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_22.h5\n",
      "439/439 [==============================] - 925s 2s/step - loss: 0.1662 - accuracy: 0.9318 - val_loss: 1.0034 - val_accuracy: 0.7883\n",
      "Epoch 23/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.1678 - accuracy: 0.9368\n",
      "Epoch 23: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_23.h5\n",
      "439/439 [==============================] - 924s 2s/step - loss: 0.1678 - accuracy: 0.9368 - val_loss: 0.9369 - val_accuracy: 0.7950\n",
      "Epoch 24/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.1574 - accuracy: 0.9353\n",
      "Epoch 24: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_24.h5\n",
      "439/439 [==============================] - 944s 2s/step - loss: 0.1574 - accuracy: 0.9353 - val_loss: 0.9761 - val_accuracy: 0.7683\n",
      "Epoch 25/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.1477 - accuracy: 0.9444\n",
      "Epoch 25: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_25.h5\n",
      "439/439 [==============================] - 914s 2s/step - loss: 0.1477 - accuracy: 0.9444 - val_loss: 0.8386 - val_accuracy: 0.7967\n",
      "Epoch 26/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.1366 - accuracy: 0.9457\n",
      "Epoch 26: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_26.h5\n",
      "439/439 [==============================] - 920s 2s/step - loss: 0.1366 - accuracy: 0.9457 - val_loss: 0.9332 - val_accuracy: 0.7767\n",
      "Epoch 27/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.9477\n",
      "Epoch 27: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_27.h5\n",
      "439/439 [==============================] - 924s 2s/step - loss: 0.1315 - accuracy: 0.9477 - val_loss: 1.0846 - val_accuracy: 0.7717\n",
      "Epoch 28/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9498\n",
      "Epoch 28: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_28.h5\n",
      "439/439 [==============================] - 937s 2s/step - loss: 0.1309 - accuracy: 0.9498 - val_loss: 1.1267 - val_accuracy: 0.7567\n",
      "Epoch 29/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.1192 - accuracy: 0.9559\n",
      "Epoch 29: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_29.h5\n",
      "439/439 [==============================] - 939s 2s/step - loss: 0.1192 - accuracy: 0.9559 - val_loss: 1.1432 - val_accuracy: 0.7450\n",
      "Epoch 30/30\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.1109 - accuracy: 0.9574\n",
      "Epoch 30: saving model to Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_30.h5\n",
      "439/439 [==============================] - 937s 2s/step - loss: 0.1109 - accuracy: 0.9574 - val_loss: 1.1035 - val_accuracy: 0.7817\n",
      "CPU times: user 10h 28min 9s, sys: 4h 56min 30s, total: 15h 24min 39s\n",
      "Wall time: 10h 10min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TRAINING THE MODEL\n",
    "# We supply test data as validation data to observe the model performance after each epoch\n",
    "# 30 iterations (epochs)\n",
    "# Uses class weight balancing based on class imbalance in data (no class imbalance here for oversampling)\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test), batch_size=32, callbacks=[checkpoint], class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and compile a saved weight\n",
    "model.load_weights('Glove_ORIGINAL_DATA__lstm_weights_trainableFalse_lr0.001_recdropouts0.2_gradientclip1_classweights_epoch_05.h5')\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "wmYpAQLx9FJt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 6s 314ms/step - loss: 0.5612 - accuracy: 0.8117\n",
      "Test Accuracy: 0.8116666674613953\n",
      "CPU times: user 8.32 s, sys: 3.34 s, total: 11.7 s\n",
      "Wall time: 6.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use the loaded weights and evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visulaising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_test is your test data\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)  # Get the predicted class labels\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming conf_matrix is the confusion matrix\n",
    "TP = np.diag(conf_matrix)\n",
    "FP = np.sum(conf_matrix, axis=0) - TP\n",
    "FN = np.sum(conf_matrix, axis=1) - TP\n",
    "\n",
    "# Avoid division by zero\n",
    "precision = np.divide(TP, (TP + FP), where=(TP + FP) != 0)\n",
    "recall = np.divide(TP, (TP + FN), where=(TP + FN) != 0)\n",
    "\n",
    "# Print precision and recall for each class\n",
    "for i in range(len(precision)):\n",
    "    print(f\"Class {i}: Precision = {precision[i]:.4f}, Recall = {recall[i]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
